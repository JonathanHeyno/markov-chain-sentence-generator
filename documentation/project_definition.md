# Markov chain sentence generator
The goal of the project is to create a program that can generate sentences based on an input of text, e.g. one or more books, poems, articles, musical lyrics, etc. Presumably the program will generate sentences in a similar style to the given input text.

## Algorithm design
Sentences are generated by drawing words according to probabilities based on previous ones through Markov chains. The probabilities of subsequent words following a given word, word couple, triple, etc. are stored in a trie structure. The trie structure has a worst-case time complexity of O(n) where n is the length of the longest word (or word pair, triple, etc.) in the input text. The space complexity is O(ALPHABET_SIZE * average key length * N) where N is the number of keys in the trie.
There are some ways to try to reduce the amount of space required by the naïve implementation, though the order will remain the same. One potential idea I might try is to first read through the input text and list for each letter what letters come immediately after it. This way when constructing the trie, we don’t need to reserve an array the size of the entire alphabet for each node in the tree since some letters usually don’t follow one another. This may also marginally speed up searching the trie. The idea is that e.g. in English you typically don’t have any word with “hb” in it anywhere since the letter b practically never follows the letter h. This will require reading through the input text twice, once to form the table telling how many child nodes each letter can have, and then a second time to form the trie. Alternatively, we could use a mutable data structure like an array or list instead of a table, I’m not sure what would be better yet.

The idea in the forementioned table with is the following:
letter | letters that can follow | number of letter that can follow
--- | --- | ---
a | a, b, c, … z |24
b | a, b, c, … y |18

Thus we know that e.g. the letter “a” can be followed by 24 different letters, the letter “b” by 18 different letters, and so on. This information is used when constructing the trie to reduce the amount of space we need to reserve for each node. At the end of each key (i.e. word, word pair, etc.) we list all the words that can follow that key, and their frequencies. So e.g. the word “cat” would be stored into the trie as below, and at the end there will be a table / dict with the frequencies of all the words (possibly also symbols) that follow it.

![trie_structure](/pics/trie_structure.svg)

## Input and output
The input is a text file with the text on which the sentence generation will be done, e.g. some book as a text file. The output will be text generated by following the Markov chain.

## Other information
- The project will be done with Python
- I can also evaluate a project done with Java
- Documentation will be in English
- Study program: Tietojenkäsittelytieteen kandidaatti (TKT)


## Sources
- Trie structures:
	- [https://www.geeksforgeeks.org/trie-insert-and-search/](https://www.geeksforgeeks.org/trie-insert-and-search/)
	- [https://iq.opengenus.org/time-complexity-of-trie/](https://iq.opengenus.org/time-complexity-of-trie/)
	- [https://en.wikipedia.org/wiki/Radix_tree]( https://en.wikipedia.org/wiki/Radix_tree)
	- [https://en.wikipedia.org/wiki/Trie]( https://en.wikipedia.org/wiki/Trie)

- Markov chains:
	- [https://brilliant.org/wiki/markov-chains/](https://brilliant.org/wiki/markov-chains/)
	- [https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75]( https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75)
	- [https://en.wikipedia.org/wiki/Hidden_Markov_model]( https://en.wikipedia.org/wiki/Hidden_Markov_model)
	- [https://en.wikipedia.org/wiki/Markov_chain]( https://en.wikipedia.org/wiki/Markov_chain)

- Part of speech tagging:
	- [https://towardsdatascience.com/part-of-speech-tagging-for-beginners-3a0754b2ebba]( https://towardsdatascience.com/part-of-speech-tagging-for-beginners-3a0754b2ebba)
	- [https://en.wikipedia.org/wiki/Viterbi_algorithm]( https://en.wikipedia.org/wiki/Viterbi_algorithm)
